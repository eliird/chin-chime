{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wave2vec = Wav2Vec2ForSequenceClassification.\\\n",
    "    from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "from typing import Tuple, Any, List\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame, filename: str, wavfile_base_path: str) -> pd.DataFrame:\n",
    "    df.dropna(inplace=True)\n",
    "    df['filename'] = filename\n",
    "    df['label'] = df['labels_male'] if filename[5] == 'M' else df['label_feamle']\n",
    "    df['filename'] = df['filename'].apply(lambda x: os.path.join(wavfile_base_path, filename + '.wav'))\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)    \n",
    "    df.drop(columns=['labels_male', 'label_feamle'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_file(path: str, wavfile_base_path: str) -> pd.DataFrame:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        df = pd.read_csv(path)\n",
    "    filename = path.split('/')[-1].split('.')[0]\n",
    "    if 'Ses01' in filename:\n",
    "        session = 'Session1'\n",
    "    elif 'Ses02' in filename:\n",
    "        session = 'Session2'\n",
    "    elif 'Ses03' in filename:\n",
    "        session = 'Session3'\n",
    "    elif 'Ses04' in filename:\n",
    "        session = 'Session4'\n",
    "    elif 'Ses05' in filename:\n",
    "        session = 'Session5'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Session  Name\")\n",
    "    \n",
    "    wavfile_base_path = wavfile_base_path.replace('SessionX', session)\n",
    "        \n",
    "    df = preprocess_df(df, filename, wavfile_base_path)\n",
    "    return df\n",
    "\n",
    "def load_files(files: List[str], wavfile_base_path: str):\n",
    "    data = []\n",
    "    for file in tqdm(files):\n",
    "        data.append(load_file(file, wavfile_base_path))\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def load_data(base_path: str, wavfile_base_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_files = [os.path.join(base_path, file) for file in os.listdir(base_path) if 'Ses05' not in file]\n",
    "    test_files = [os.path.join(base_path, file) for file in os.listdir(base_path) if 'Ses05' in file]\n",
    "    train = load_files(train_files, wavfile_base_path)\n",
    "    test = load_files(test_files, wavfile_base_path)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame#</th>\n",
       "      <th>Time</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>Z02</th>\n",
       "      <th>X03</th>\n",
       "      <th>Y03</th>\n",
       "      <th>...</th>\n",
       "      <th>Y53</th>\n",
       "      <th>Z53</th>\n",
       "      <th>X60</th>\n",
       "      <th>Y60</th>\n",
       "      <th>Z60</th>\n",
       "      <th>X61</th>\n",
       "      <th>Y61</th>\n",
       "      <th>Z61</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>909</td>\n",
       "      <td>7.58266</td>\n",
       "      <td>-27.56770</td>\n",
       "      <td>34.97618</td>\n",
       "      <td>-54.40529</td>\n",
       "      <td>-0.23704</td>\n",
       "      <td>25.24322</td>\n",
       "      <td>-61.08478</td>\n",
       "      <td>23.64563</td>\n",
       "      <td>36.21480</td>\n",
       "      <td>...</td>\n",
       "      <td>24.13990</td>\n",
       "      <td>-45.04360</td>\n",
       "      <td>55.95958</td>\n",
       "      <td>56.64390</td>\n",
       "      <td>121.62669</td>\n",
       "      <td>-45.31238</td>\n",
       "      <td>52.59035</td>\n",
       "      <td>129.44393</td>\n",
       "      <td>/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>910</td>\n",
       "      <td>7.59100</td>\n",
       "      <td>-27.59504</td>\n",
       "      <td>34.97534</td>\n",
       "      <td>-54.53862</td>\n",
       "      <td>-0.14238</td>\n",
       "      <td>25.29796</td>\n",
       "      <td>-61.08746</td>\n",
       "      <td>23.63664</td>\n",
       "      <td>36.28221</td>\n",
       "      <td>...</td>\n",
       "      <td>24.20146</td>\n",
       "      <td>-45.19563</td>\n",
       "      <td>55.99112</td>\n",
       "      <td>56.57716</td>\n",
       "      <td>121.61237</td>\n",
       "      <td>-45.29489</td>\n",
       "      <td>52.42889</td>\n",
       "      <td>129.39733</td>\n",
       "      <td>/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>911</td>\n",
       "      <td>7.59934</td>\n",
       "      <td>-27.62559</td>\n",
       "      <td>34.94876</td>\n",
       "      <td>-54.60670</td>\n",
       "      <td>-0.15016</td>\n",
       "      <td>25.26522</td>\n",
       "      <td>-61.18179</td>\n",
       "      <td>23.59330</td>\n",
       "      <td>36.19091</td>\n",
       "      <td>...</td>\n",
       "      <td>24.20199</td>\n",
       "      <td>-45.38675</td>\n",
       "      <td>56.02899</td>\n",
       "      <td>56.47296</td>\n",
       "      <td>121.58130</td>\n",
       "      <td>-45.28643</td>\n",
       "      <td>52.32635</td>\n",
       "      <td>129.36415</td>\n",
       "      <td>/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>912</td>\n",
       "      <td>7.60768</td>\n",
       "      <td>-27.66087</td>\n",
       "      <td>34.82575</td>\n",
       "      <td>-54.76922</td>\n",
       "      <td>-0.11912</td>\n",
       "      <td>25.22126</td>\n",
       "      <td>-61.34704</td>\n",
       "      <td>23.66278</td>\n",
       "      <td>36.07449</td>\n",
       "      <td>...</td>\n",
       "      <td>24.22021</td>\n",
       "      <td>-45.60330</td>\n",
       "      <td>56.02719</td>\n",
       "      <td>56.37018</td>\n",
       "      <td>121.55001</td>\n",
       "      <td>-45.29266</td>\n",
       "      <td>52.30411</td>\n",
       "      <td>129.36309</td>\n",
       "      <td>/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>913</td>\n",
       "      <td>7.61602</td>\n",
       "      <td>-27.59027</td>\n",
       "      <td>34.67632</td>\n",
       "      <td>-54.79646</td>\n",
       "      <td>-0.14865</td>\n",
       "      <td>25.16280</td>\n",
       "      <td>-61.38764</td>\n",
       "      <td>23.70279</td>\n",
       "      <td>35.97943</td>\n",
       "      <td>...</td>\n",
       "      <td>24.12938</td>\n",
       "      <td>-45.68056</td>\n",
       "      <td>56.03284</td>\n",
       "      <td>56.31125</td>\n",
       "      <td>121.55555</td>\n",
       "      <td>-45.30947</td>\n",
       "      <td>52.35530</td>\n",
       "      <td>129.35845</td>\n",
       "      <td>/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Frame#     Time       X01       Y01       Z01      X02       Y02  \\\n",
       "908     909  7.58266 -27.56770  34.97618 -54.40529 -0.23704  25.24322   \n",
       "909     910  7.59100 -27.59504  34.97534 -54.53862 -0.14238  25.29796   \n",
       "910     911  7.59934 -27.62559  34.94876 -54.60670 -0.15016  25.26522   \n",
       "911     912  7.60768 -27.66087  34.82575 -54.76922 -0.11912  25.22126   \n",
       "912     913  7.61602 -27.59027  34.67632 -54.79646 -0.14865  25.16280   \n",
       "\n",
       "          Z02       X03       Y03  ...       Y53       Z53       X60  \\\n",
       "908 -61.08478  23.64563  36.21480  ...  24.13990 -45.04360  55.95958   \n",
       "909 -61.08746  23.63664  36.28221  ...  24.20146 -45.19563  55.99112   \n",
       "910 -61.18179  23.59330  36.19091  ...  24.20199 -45.38675  56.02899   \n",
       "911 -61.34704  23.66278  36.07449  ...  24.22021 -45.60330  56.02719   \n",
       "912 -61.38764  23.70279  35.97943  ...  24.12938 -45.68056  56.03284   \n",
       "\n",
       "          Y60        Z60       X61       Y61        Z61  \\\n",
       "908  56.64390  121.62669 -45.31238  52.59035  129.44393   \n",
       "909  56.57716  121.61237 -45.29489  52.42889  129.39733   \n",
       "910  56.47296  121.58130 -45.28643  52.32635  129.36415   \n",
       "911  56.37018  121.55001 -45.29266  52.30411  129.36309   \n",
       "912  56.31125  121.55555 -45.30947  52.35530  129.35845   \n",
       "\n",
       "                                              filename    label  \n",
       "908  /media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...  Neutral  \n",
       "909  /media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...  Neutral  \n",
       "910  /media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...  Neutral  \n",
       "911  /media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...  Neutral  \n",
       "912  /media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_rel...  Neutral  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './iemocap_processed/merged/Ses01F_impro01.csv'\n",
    "wav_file_path_template = '/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_release/SessionX/dialog/wav'\n",
    "\n",
    "df = load_file(path, wav_file_path_template)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './iemocap_processed/merged/'\n",
    "wav_file_path_template = '/media/cv/Extreme Pro/IEMOCAP/IEMOCAP_full_release/SessionX/dialog/wav'\n",
    "\n",
    "# train, test = load_data(base_path, wav_file_path_template)\n",
    "\n",
    "save_train = './iemocap_processed/pickled/train_with_audio_file.pkl'\n",
    "save_test = './iemocap_processed/pickled/test_with_audio_file.pkl'\n",
    "# train.to_pickle(save_train)\n",
    "# test.to_pickle(save_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_train = './iemocap_processed/pickled/train_with_audio_file.pkl'\n",
    "save_test = './iemocap_processed/pickled/test_with_audio_file.pkl'\n",
    "\n",
    "train_data = pd.read_pickle(save_train)\n",
    "test_data = pd.read_pickle(save_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, audio_processor, sr=16_000, duration: float=2):\n",
    "        self.df = df\n",
    "        self.df = self.df.loc[ self.df['label'] != 'Other']\n",
    "        \n",
    "        self.label2id = {\n",
    "            'Frustration':0,\n",
    "            'Anger':1,\n",
    "            'Excited':2,\n",
    "            'Neutral':3,\n",
    "            'Happiness':4,\n",
    "            'Sadness':5,\n",
    "            'Fear':6,\n",
    "            'Surprise':7,\n",
    "        }\n",
    "\n",
    "        self.id2label = {\n",
    "            0: 'Frustration',\n",
    "            1: 'Anger',\n",
    "            2: 'Excited',\n",
    "            3: 'Neutral',\n",
    "            4: 'Happiness',\n",
    "            5: 'Sadness',\n",
    "            6: 'Fear',\n",
    "            7: 'Surprise',\n",
    "        }\n",
    "\n",
    "\n",
    "        self.processor = audio_processor\n",
    "        self.duration = duration\n",
    "        self.sampling_rate = sr\n",
    "        self.samples = self.duration * self.sampling_rate\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        file_path = self.df.iloc[index]['filename']\n",
    "        offset = max(0, self.df.iloc[index]['Time'] - self.duration)\n",
    "    \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            audio_input ,sr = lb.load(file_path, sr=self.sampling_rate, mono=True, offset=offset, duration=self.duration)\n",
    "        audio_input = self.pad_or_truncate(audio_input)\n",
    "        input_values = self.processor(audio_input, sampling_rate=sr, return_tensors='pt').input_values\n",
    "        labels = torch.tensor(self.label2id[self.df.iloc[index]['label']])\n",
    "        \n",
    "        return (input_values.squeeze(), labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_truncate(self, arr):\n",
    "        if len(arr) > self.samples:\n",
    "            arr = arr[:self.samples]\n",
    "        else:\n",
    "            arr = np.pad(arr, (0, self.samples - len(arr)), 'constant', constant_values=(0))\n",
    "        return arr\n",
    "\n",
    "def tensor_data_to_dataloader(dataset: TensorDataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        sampler=RandomSampler(dataset)\n",
    "    )\n",
    "\n",
    "def build_dataset(pickle_path: str, audio_processor, batch_size=32, sampling_frac=0.5):\n",
    "    data = pd.read_pickle(pickle_path)\n",
    "    data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n",
    "    data = tensor_data_to_dataloader(AudioDataset(data, audio_processor), batch_size=batch_size)\n",
    "    return data\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "trainloader = build_dataset(save_train, processor, batch_size=batch_size)\n",
    "testloader = build_dataset(save_test, processor, batch_size=batch_size)\n",
    "# i, temp = next(enumerate(train_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, temp = next(enumerate(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import datetime, time, random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "class TrainingArgs:\n",
    "    def __init__(self, device='cuda', learning_rate=2e-5, epsilon=1e-8, epochs=4) -> None:\n",
    "        self.device = device\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.warmup_steps = 0\n",
    "        self.seed = 1024\n",
    "\n",
    "    \n",
    "class TrainerWave2Vec:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 trainloader: DataLoader,\n",
    "                 testloader: DataLoader,\n",
    "                 out_dir: str,\n",
    "                 args: TrainingArgs) -> None:\n",
    "        \n",
    "        self.args = args\n",
    "        self.num_epochs = self.args.epochs\n",
    "        self.device = self.args.device\n",
    "        self.out_dir = out_dir\n",
    "        \n",
    "        self.train_data = trainloader\n",
    "        self.val_data = testloader\n",
    "\n",
    "        self.best_acc = 0\n",
    "        \n",
    "        self.training_stats = []\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = AdamW(model.parameters(), lr=self.args.lr, eps=self.args.epsilon)\n",
    "        self.schedular = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps= self.args.warmup_steps, num_training_steps=len(trainloader) * self.args.epochs            \n",
    "        )\n",
    "        \n",
    "        self.fix_seeds(self.args.seed)\n",
    "    \n",
    "    def fix_seeds(self, seed_val):\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        \n",
    "    def validate(self):\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        self.model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        \n",
    "        preds = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, batch in enumerate(tqdm(self.val_data)):\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                output= self.model(b_input_ids, \n",
    "                                    labels=b_labels)\n",
    "            loss = output.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU if we are using GPU\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.cpu().numpy()\n",
    "            \n",
    "            preds.extend(np.argmax(logits, axis=1))\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        preds, labels = np.array(preds), np.array(labels)\n",
    "        avg_val_accuracy = f1_score(preds.flatten(), labels.flatten(), average='weighted')# total_eval_accuracy / len(dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(self.val_data)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        if avg_val_accuracy > self.best_acc:\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.out_dir, 'wave2vec.pt'))\n",
    "            self.best_acc = avg_val_accuracy\n",
    "\n",
    "        return (avg_val_accuracy, avg_val_loss, validation_time)\n",
    "    \n",
    "    def train_epoch(self, epoch_i):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.num_epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        self.model.train()\n",
    "        for step, batch in enumerate(tqdm(self.train_data)):\n",
    "        \n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_labels = batch[1].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(b_input_ids, \n",
    "                                labels=b_labels)        \n",
    "            loss = output.loss\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "    \n",
    "            self.optimizer.step()\n",
    "            self.schedular.step()\n",
    "            \n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(self.train_data)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))    \n",
    "        return (avg_train_loss, training_time)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            train_loss, train_time = self.train_epoch(epoch)\n",
    "            val_acc, val_loss, val_time = self.validate()\n",
    "            self.training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'Training Loss': train_loss,\n",
    "                    'Valid. Loss': val_loss,\n",
    "                    'Valid. Accur.': val_acc,\n",
    "                    'Training Time': train_time,\n",
    "                    'Validation Time': val_time\n",
    "                }\n",
    "            )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = TrainerWave2Vec(\n",
    "    model=wave2vec,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    out_dir='./saved_weights_iemocap_audio',\n",
    "    args=TrainingArgs()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_602194/2499914204.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n",
      "/tmp/ipykernel_602194/2499914204.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n"
     ]
    }
   ],
   "source": [
    "class AudioLandmarkDataset(Dataset):\n",
    "    def __init__(self, df, audio_processor, sr=16_000, duration: float=2):\n",
    "        self.df = df\n",
    "        self.df = self.df.loc[ self.df['label'] != 'Other']\n",
    "        \n",
    "        self.label2id = {\n",
    "            'Frustration':0,            'Anger':1,            'Excited':2,            'Neutral':3,\n",
    "            'Happiness':4,            'Sadness':5,            'Fear':6,            'Surprise':7,\n",
    "        }\n",
    "\n",
    "        self.id2label = {\n",
    "            0: 'Frustration',            1: 'Anger',            2: 'Excited',            3: 'Neutral',\n",
    "            4: 'Happiness',            5: 'Sadness',            6: 'Fear',            7: 'Surprise',\n",
    "        }\n",
    "        \n",
    "        self.columns_to_extract = [\n",
    "            'X15', 'Y15', 'Z15', 'X16', 'Y16', 'Z16', 'X18', 'Y18', 'Z18', 'X07',\n",
    "            'Y07', 'Z07', 'X08', 'Y08', 'Z08', 'X10', 'Y10', 'Z10', 'X01', 'Y01',\n",
    "            'Z01', 'X02', 'Y02', 'Z02', 'X03', 'Y03', 'Z03', 'X46', 'Y46', 'Z46',\n",
    "            'X47', 'Y47', 'Z47', 'X48', 'Y48', 'Z48', 'X49', 'Y49', 'Z49', 'X50',\n",
    "            'Y50', 'Z50', 'X51', 'Y51', 'Z51', 'X52', 'Y52', 'Z52', 'X53', 'Y53',\n",
    "            'Z53', 'X29', 'Y29', 'Z29', 'X27', 'Y27', 'Z27', 'X28', 'Y28', 'Z28'\n",
    "            ]\n",
    "        \n",
    "\n",
    "        self.processor = audio_processor\n",
    "        self.duration = duration\n",
    "        self.sampling_rate = sr\n",
    "        self.samples = self.duration * self.sampling_rate\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        file_path = self.df.iloc[index]['filename']\n",
    "        offset = max(0, self.df.iloc[index]['Time'] - self.duration)\n",
    "    \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            audio_input ,sr = lb.load(file_path, sr=self.sampling_rate, mono=True, offset=offset, duration=self.duration)\n",
    "        audio_input = self.pad_or_truncate(audio_input)\n",
    "        input_values = self.processor(audio_input, sampling_rate=sr, return_tensors='pt').input_values\n",
    "        labels = torch.tensor(self.label2id[self.df.iloc[index]['label']])\n",
    "        pose = torch.tensor(self.df.iloc[index][self.columns_to_extract].astype(float).to_numpy(), dtype=torch.float32)\n",
    "        \n",
    "        return (input_values.squeeze(), pose, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_truncate(self, arr):\n",
    "        if len(arr) > self.samples:\n",
    "            arr = arr[:self.samples]\n",
    "        else:\n",
    "            arr = np.pad(arr, (0, self.samples - len(arr)), 'constant', constant_values=(0))\n",
    "        return arr\n",
    "\n",
    "def tensor_data_to_dataloader(dataset: TensorDataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        sampler=RandomSampler(dataset)\n",
    "    )\n",
    "\n",
    "def build_dataset(pickle_path: str, audio_processor, batch_size=32, sampling_frac=0.5):\n",
    "    data = pd.read_pickle(pickle_path)\n",
    "    data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n",
    "    data = tensor_data_to_dataloader(AudioLandmarkDataset(data, audio_processor), batch_size=batch_size)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class MLPLandmark(nn.Module):\n",
    "    def __init__(self, inp_dim: int, out_dim: int, layers: list):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        self.mlp = nn.ModuleList()\n",
    "        \n",
    "        self.mlp.append(nn.Linear(self.inp_dim, layers[0]))\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            self.mlp.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layers[i-1], layers[i]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LayerNorm(layers[i])\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.mlp.append(nn.Linear(layers[-1], out_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.view(x.shape[0], self.inp_dim)\n",
    "        \n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class LandmarkAudioModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_dim_landmark : int,\n",
    "                 feat_dim: int = 256,\n",
    "                 layers: List[int] = [128, 256, 512],\n",
    "                 audio_model_path: str = \"facebook/wav2vec2-base-960h\",\n",
    "                 num_labels: int = 8,\n",
    "                 criterion=nn.CrossEntropyLoss()):\n",
    "        super().__init__()\n",
    "        self.audio_model_path = audio_model_path\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.landmark_model = MLPLandmark(inp_dim_landmark , feat_dim, layers)\n",
    "        \n",
    "        self.audio_model = wave2vec = Wav2Vec2ForSequenceClassification.\\\n",
    "                                        from_pretrained(audio_model_path, num_labels=feat_dim)\n",
    "                                        \n",
    "        self.fc = nn.Linear(2*feat_dim, num_labels)\n",
    "\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, audio_input, landmarks, labels=None):\n",
    "        audio = self.audio_model(audio_input).logits\n",
    "        text = self.landmark_model(landmarks)\n",
    "\n",
    "        out = self.fc(torch.cat((audio, text), dim=1))\n",
    "        print(out.shape)\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = self.criterion(out, labels)\n",
    "\n",
    "        return {'logits': out, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgs:\n",
    "    def __init__(self, device='cuda', learning_rate=2e-5, epsilon=1e-8, epochs=4) -> None:\n",
    "        self.device = device\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.warmup_steps = 0\n",
    "        self.seed = 1024\n",
    "\n",
    "    \n",
    "class TrainerWave2Vec:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 trainloader: DataLoader,\n",
    "                 testloader: DataLoader,\n",
    "                 out_dir: str,\n",
    "                 args: TrainingArgs) -> None:\n",
    "        \n",
    "        self.args = args\n",
    "        self.num_epochs = self.args.epochs\n",
    "        self.device = self.args.device\n",
    "        self.out_dir = out_dir\n",
    "        \n",
    "        self.train_data = trainloader\n",
    "        self.val_data = testloader\n",
    "\n",
    "        self.best_acc = 0\n",
    "        \n",
    "        self.training_stats = []\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = AdamW(model.parameters(), lr=self.args.lr, eps=self.args.epsilon)\n",
    "        self.schedular = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps= self.args.warmup_steps, num_training_steps=len(trainloader) * self.args.epochs            \n",
    "        )\n",
    "        \n",
    "        self.fix_seeds(self.args.seed)\n",
    "    \n",
    "    def fix_seeds(self, seed_val):\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        \n",
    "    def validate(self):\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        self.model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        \n",
    "        preds = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, batch in enumerate(tqdm(self.val_data)):\n",
    "            b_audio = batch[0].to(self.device)\n",
    "            b_landmarks = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                output= self.model(b_audio, b_landmarks, \n",
    "                                    labels=b_labels)\n",
    "            loss = output['loss']\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU if we are using GPU\n",
    "            logits = output['logits']\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.cpu().numpy()\n",
    "            \n",
    "            preds.extend(np.argmax(logits, axis=1))\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        preds, labels = np.array(preds), np.array(labels)\n",
    "        avg_val_accuracy = f1_score(preds.flatten(), labels.flatten(), average='weighted')# total_eval_accuracy / len(dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(self.val_data)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        if avg_val_accuracy > self.best_acc:\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.out_dir, 'wave2vec.pt'))\n",
    "            self.best_acc = avg_val_accuracy\n",
    "\n",
    "        return (avg_val_accuracy, avg_val_loss, validation_time)\n",
    "    \n",
    "    def train_epoch(self, epoch_i):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.num_epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        self.model.train()\n",
    "        for step, batch in enumerate(tqdm(self.train_data)):\n",
    "        \n",
    "            b_audio = batch[0].to(self.device)\n",
    "            b_landmarks = batch[1].to(self.device)\n",
    "            \n",
    "            b_labels = batch[1].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            print(b_audio.shape, b_landmarks.shape, b_labels.shape)\n",
    "            output = self.model(b_audio, b_landmarks, \n",
    "                                labels=b_labels)  \n",
    "            print(output.shape)      \n",
    "            loss = output['loss']\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "    \n",
    "            self.optimizer.step()\n",
    "            self.schedular.step()\n",
    "            \n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(self.train_data)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))    \n",
    "        return (avg_train_loss, training_time)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            train_loss, train_time = self.train_epoch(epoch)\n",
    "            val_acc, val_loss, val_time = self.validate()\n",
    "            self.training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'Training Loss': train_loss,\n",
    "                    'Valid. Loss': val_loss,\n",
    "                    'Valid. Accur.': val_acc,\n",
    "                    'Training Time': train_time,\n",
    "                    'Validation Time': val_time\n",
    "                }\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_602194/2499914204.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n",
      "/tmp/ipykernel_602194/2499914204.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=sampling_frac))\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "trainloader = build_dataset(save_train, processor, batch_size=batch_size)\n",
    "testloader = build_dataset(save_test, processor, batch_size=batch_size)\n",
    "model = LandmarkAudioModel(inp_dim_landmark=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerWave2Vec(\n",
    "    model=wave2vec,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    out_dir='./saved_weights_iemocap_audio_text',\n",
    "    args=TrainingArgs()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32000]) torch.Size([32, 60]) torch.Size([32, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7651 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (1920).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 133\u001b[0m, in \u001b[0;36mTrainerWave2Vec.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[0;32m--> 133\u001b[0m         train_loss, train_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         val_acc, val_loss, val_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    136\u001b[0m             {\n\u001b[1;32m    137\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m             }\n\u001b[1;32m    144\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[42], line 109\u001b[0m, in \u001b[0;36mTrainerWave2Vec.train_epoch\u001b[0;34m(self, epoch_i)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(b_audio\u001b[38;5;241m.\u001b[39mshape, b_landmarks\u001b[38;5;241m.\u001b[39mshape, b_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 109\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_landmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_labels\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)      \n\u001b[1;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2131\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2130\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 2131\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   2134\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[_HIDDEN_STATES_START_POSITION:]\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tte/lib/python3.9/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (1920)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
